{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../archives/crec.db')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move data from database to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crec_df = pd.read_sql(\"Select * from crec\", conn, index_col='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crec_df[\"html_data\"] = crec_df[\"html_data\"].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crec_df = crec_df.set_index(pd.DatetimeIndex(crec_df.index))#.ix[:'2016-12-31'] #control date span of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = crec_df[crec_df.html_data.str.contains(r'climate\\schange|global\\swarming')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.set_index(pd.DatetimeIndex(reduced_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data for natural language analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "raw_reduced = reduced_df.html_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(str(raw_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_reduced = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crec_df)  # total number of entries in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reduced_df)  # total number of entries in keyword-limited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_reduced)  # total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall(r'(climate\\schange|global\\swarming)', str(raw_reduced), re.IGNORECASE)) # total words matching query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic NLKT Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_reduced.concordance(\"climate\", lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram-based Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bigram_list = []\n",
    "for bg in list(nltk.bigrams(tokenized_reduced)):\n",
    "    if 'climate' in bg or 'warming' in bg:\n",
    "        if bg[0] not in stopwords and bg[1] not in stopwords:\n",
    "            bigram_list.append(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bigrams_tagged = []\n",
    "for i in bigram_list:\n",
    "    bigrams_tagged.append(nltk.pos_tag(i, tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(bigram_list)\n",
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tagged_list =[]\n",
    "for i in bigrams_tagged:\n",
    "    if i[0][1] == 'VERB' or i[1][1] == 'VERB':\n",
    "        tagged_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_list = []\n",
    "for sublist in tagged_list:\n",
    "    for sub_sublist in sublist:\n",
    "        POS_list.append(sub_sublist)\n",
    "POS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(POS_list).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex-Based Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combat/Agency Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = r'\\b(climate\\schange|global\\swarming)\\W+(?:\\w+\\W+){0,150}?(fight(ing)?|(battle|battling)|must act|combat(ing)?|(struggle|struggling)|(oppose|opposing)|fight(ing)?\\sback|defend(ing?)|press(ing)?|push(ing)?|campaign(ing)?)|(fight(ing)?|(battle|battling)|must act|combat(ing)?|(struggle|struggling)|(oppose|opposing)|fight(ing)?\\sback|defend(ing?)|press(ing)?|push(ing)?|campaign(ing)?)\\W+(?:\\w+\\W+){0,150}?(climate\\schange|global\\swarming)\\b'\n",
    "query1_desc = '\"Climate Change\" collocated with combat terms'\n",
    "query2 = r'\\b(climate\\schange|global\\swarming)\\W+(?:\\w+\\W+){0,150}?((examine|examining)|study(ing)?|assess(ing)?|model(ing)?|(measure|measuring)|(evaluate|evaluating)|(appraise|appraising))|((examine|examining)|study(ing)?|assess(ing)?|model(ing)?|(measure|measuring)|(evaluate|evaluating)|(appraise|appraising))\\W+(?:\\w+\\W+){0,150}?(climate\\schange|global\\swarming)\\b'\n",
    "query2_desc = '\"Climate Change\" collocated with assessment terms'\n",
    "query3 = r'\\b(climate\\schange|global\\swarming)\\W+(?:\\w+\\W+){0,150}?(man-made|anthropogenic|human-caused|cause(d|s)?)|(man-made|anthropogenic|human-caused|cause(d)?)\\W+(?:\\w+\\W+){0,150}?(climate\\schange|global\\swarming)\\b'\n",
    "query3_desc = 'Agentic Ratio/Human Agency Foregrounded/Culpability Foregrounded'\n",
    "query4 = r'\\bc(climate\\schange|global\\swarming)\\W+(?:\\w+\\W+){0,150}?(nature|natural|cycle|cyclical|slow)|(nature|natural|cycle|cyclical|slow)\\W+(?:\\w+\\W+){0,150}?(climate\\schange|global\\swarming)\\b'\n",
    "query4_desc = 'Scenic Ratio/Nature Foregrounded/Culpability Backgrounded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: the regex queries in the cell above may be older/different than those in the final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1_df = pd.DataFrame(index=reduced_df.index, data=reduced_df.html_data.str.count(query1, re.IGNORECASE))\n",
    "query1_df.columns = [query1_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_df = pd.DataFrame(index=reduced_df.index, data=reduced_df.html_data.str.count(query2, re.IGNORECASE))\n",
    "query2_df.columns = [query2_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "query3_df = pd.DataFrame(index=reduced_df.index, data=reduced_df.html_data.str.count(query3, re.IGNORECASE))\n",
    "query3_df.columns = [query3_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "query4_df = pd.DataFrame(index=reduced_df.index, data=reduced_df.html_data.str.count(query4, re.IGNORECASE))\n",
    "query4_df.columns = [query4_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "query5_df = pd.DataFrame(index=reduced_df.index, data=reduced_df.html_data.str.count(query4, re.IGNORECASE))\n",
    "query5_df.columns = [query4_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(query1_df.groupby(lambda x:x.year).sum())\n",
    "print(query2_df.groupby(lambda x:x.year).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,13))\n",
    "\n",
    "ax = fig1.add_subplot(211)\n",
    "ax.set_xlabel('Year', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Agentic Frames', fontweight='bold')\n",
    "plot1=plt.plot(query1_df.groupby(lambda x:x.year).sum(), '--')\n",
    "plot2=plt.plot(query2_df.groupby(lambda x:x.year).sum(), '-')\n",
    "plt.legend((query1_desc, query2_desc), loc=2, fontsize=15)\n",
    "\n",
    "ax = fig1.add_subplot(212)\n",
    "ax.set_xlabel('Year', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Pentadic Frames/Ratios', fontweight='bold')\n",
    "plot3=plt.plot(query3_df.groupby(lambda x:x.year).sum(), '--')\n",
    "plot4=plt.plot(query4_df.groupby(lambda x:x.year).sum())\n",
    "plt.legend((query3_desc, query4_desc), loc=2, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(15,13))\n",
    "\n",
    "plt.subplot(211)\n",
    "plot1=plt.scatter(query1_df.index, query1_df['{}'.format(query1_desc)], alpha=0.5, color=\"#FF5500\", s=query1_df['{}'.format(query1_desc)]*11)\n",
    "plot2=plt.scatter(query2_df.index, query2_df['{}'.format(query2_desc)], alpha=0.3, color=\"#6495ED\", s=query2_df['{}'.format(query2_desc)]*11)\n",
    "plt.legend((plot1, plot2), (query1_desc, query2_desc), loc=2, fontsize=17)\n",
    "\n",
    "plt.subplot(212)\n",
    "plot3=plt.scatter(query3_df.index, query3_df['{}'.format(query3_desc)], alpha=0.5, color=\"#00FF99\", s=query3_df['{}'.format(query3_desc)]*11)\n",
    "plot4=plt.scatter(query4_df.index, query4_df['{}'.format(query4_desc)], alpha=0.3, color=\"#6495ED\", s=query4_df['{}'.format(query4_desc)]*11)\n",
    "plt.legend((plot3, plot4), (query3_desc, query4_desc), loc=2, fontsize=17)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
